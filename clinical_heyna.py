# -*- coding: utf-8 -*-
"""clinical_heyna.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kZ6XDN6yM5FjW_fBRxUrwoyjT5NUK5i1
"""

from google.colab import drive
drive.mount('/content/drive')

"""*** Credit: Notebook adapted from [HyenaDNA](https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL?usp=sharing#scrollTo=OJHCegJvtASc)."""

!pip install torchvision
!pip install transformers
!pip install einops
!pip install OmegaConf

#@title Imports
import torch
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from functools import partial
from einops import rearrange
from typing import Optional
from functools import partial
from torch import Tensor
from torchvision.ops import StochasticDepth
from collections import namedtuple
import yaml
from random import random
import numpy as np
from torch.utils.data import DataLoader
import pandas as pd
from random import random
import numpy as np
from torch.utils.data import DataLoader
import pandas as pd
import torch.optim as optim
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
import copy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# Architecture

"""

#@title Hyena layer


def fftconv(u, k, D):
    """
    We apply a convolution through the fourier domain (from the Convolution Theorem)

    """
    seqlen = u.shape[-1]
    fft_size = 2 * seqlen

    k_f = torch.fft.rfft(k, n=fft_size) / fft_size
    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)

    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)
    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]

    out = y + u * D.unsqueeze(-1)
    return out.to(dtype=u.dtype)


@torch.jit.script
def mul_sum(q, y):
    return (q * y).sum(dim=1)

class OptimModule(nn.Module):
    """ Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters """

    def register(self, name, tensor, lr=None, wd=0.0):
        """Register a tensor with a configurable learning rate and 0 weight decay"""

        if lr == 0.0:
            self.register_buffer(name, tensor)
        else:
            self.register_parameter(name, nn.Parameter(tensor))

            optim = {}
            if lr is not None: optim["lr"] = lr
            if wd is not None: optim["weight_decay"] = wd
            setattr(getattr(self, name), "_optim", optim)


class Sin(nn.Module):
    """The Sin activation function for the Hyena Filter function."""
    def __init__(self, dim, w=10, train_freq=True):
        super().__init__()
        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)

    def forward(self, x):
        return torch.sin(self.freq * x)


class PositionalEmbedding(OptimModule):
    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs):
        """Complex exponential positional embeddings for Hyena filters."""
        super().__init__()

        self.seq_len = seq_len
        # The time embedding fed to the filteres is normalized so that t_f = 1
        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1

        if emb_dim > 1:
            bands = (emb_dim - 1) // 2
        # To compute the right embeddings we use the "proper" linspace
        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]
        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1

        f = torch.linspace(1e-4, bands - 1, bands)[None, None]
        z = torch.exp(-1j * f * w)
        z = torch.cat([t, z.real, z.imag], dim=-1)
        self.register("z", z, lr=lr_pos_emb)
        self.register("t", t, lr=0.0)

    def forward(self, L):
        return self.z[:, :L], self.t[:, :L]


class ExponentialModulation(OptimModule):
    """The window function applied to the output of the (MLP) filter function."""
    def __init__(
        self,
        d_model,
        fast_decay_pct=0.3,
        slow_decay_pct=1.5,
        target=1e-2,
        modulation_lr=0.0,
        modulate: bool=True,
        shift: float = 0.05,
        **kwargs
    ):
        super().__init__()
        self.modulate = modulate
        self.shift = shift
        max_decay = math.log(target) / fast_decay_pct
        min_decay = math.log(target) / slow_decay_pct
        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]
        self.register("deltas", deltas, lr=modulation_lr)

    def forward(self, t, x):
        if self.modulate:
            decay = torch.exp(-t * self.deltas.abs())
            x = x * (decay + self.shift)
        return x


class HyenaFilter(OptimModule):
    def __init__(
            self,
            d_model,
            emb_dim=3, # dim of input to MLP, augments with positional encoding
            order=16, # width of the implicit MLP
            fused_fft_conv=False,
            seq_len=1024,
            lr=1e-3,
            lr_pos_emb=1e-5,
            dropout=0.0,
            w=1, # frequency of periodic activations
            wd=0, # weight decay of kernel parameters
            bias=True,
            num_inner_mlps=2,
            normalized=False,
            **kwargs
        ):
        """
        Implicit long filter with modulation.

        Args:
            d_model: number of channels in the input
            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands
            order: width of the FFN
            num_inner_mlps: number of inner linear layers inside filter MLP

        Note:
            filter_dropout is not implemented
        """
        super().__init__()

        self.d_model = d_model
        self.use_bias = bias
        self.fused_fft_conv = fused_fft_conv
        self.bias = nn.Parameter(torch.randn(self.d_model))
        self.dropout = nn.Dropout(dropout)

        act = Sin(dim=order, w=w)
        self.emb_dim = emb_dim
        assert emb_dim % 2 != 0 and emb_dim >= 3, "emb_dim must be odd and greater or equal to 3 (time, sine and cosine)"
        self.seq_len = seq_len

        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)

        self.implicit_filter = nn.Sequential(
            nn.Linear(emb_dim, order),
            act,
        )
        for i in range(num_inner_mlps):
            self.implicit_filter.append(nn.Linear(order, order))
            self.implicit_filter.append(act)

        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))

        self.modulation = ExponentialModulation(d_model, **kwargs)

        self.normalized = normalized
        for c in self.implicit_filter.children():
            for name, v in c.state_dict().items():
                optim = {"weight_decay": wd, "lr": lr}
                setattr(getattr(c, name), "_optim", optim)

    def filter(self, L, *args, **kwargs):
        z, t = self.pos_emb(L)
        h = self.implicit_filter(z)
        h = self.modulation(t, h)
        return h

    def forward(self, x, L, k=None, bias=None, *args, **kwargs):
        if k is None: k = self.filter(L)

        # Ensure compatibility with filters that return a tuple
        k = k[0] if type(k) is tuple else k

        y = fftconv(x, k, bias)
        return y


class HyenaOperator(nn.Module):
    def __init__(
            self,
            d_model,
            l_max,
            order=2,
            filter_order=64,
            dropout=0.0,
            filter_dropout=0.0,
            **filter_args,
        ):
        r"""
        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf

        Args:
            d_model (int): Dimension of the input and output embeddings (width of the layer)
            l_max: (int): Maximum input sequence length. Defaults to None
            order: (int): Depth of the Hyena recurrence. Defaults to 2
            dropout: (float): Dropout probability. Defaults to 0.0
            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0
        """
        super().__init__()

        self.d_model = d_model
        self.l_max = l_max
        self.order = order
        inner_width = d_model * (order + 1)
        self.dropout = nn.Dropout(dropout)
        self.in_proj = nn.Linear(d_model, inner_width)
        self.out_proj = nn.Linear(d_model, d_model)

        self.short_filter = nn.Conv1d(
            inner_width,
            inner_width,
            3,
            padding=2,
            groups=inner_width
        )
        self.filter_fn = HyenaFilter(
            d_model * (order - 1),
            order=filter_order,
            seq_len=l_max,
            channels=1,
            dropout=filter_dropout,
            **filter_args
        )

    def forward(self, u, *args, **kwargs):
        l = u.size(-2)
        l_filter = min(l, self.l_max)
        u = self.in_proj(u)
        u = rearrange(u, 'b l d -> b d l')

        uc = self.short_filter(u)[...,:l_filter]
        *x, v = uc.split(self.d_model, dim=1)

        k = self.filter_fn.filter(l_filter)[0]
        k = rearrange(k, 'l (o d) -> o d l', o=self.order - 1)
        bias = rearrange(self.filter_fn.bias, '(o d) -> o d', o=self.order - 1)

        for o, x_i in enumerate(reversed(x[1:])):
            v = self.dropout(v * x_i)
            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o])

        y = rearrange(v * x[0], 'b d l -> b l d')

        y = self.out_proj(y)
        return y

#@title MLP layer

"""
The MLP layer after the mixer layer (HyenaOperator).
"""

class Mlp(nn.Module):

    def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,
                 return_residual=False, device=None, dtype=None):
        """
        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/mlp.py
        """
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.return_residual = return_residual
        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)
        self.activation = activation
        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)

    def forward(self, x):
        y = self.fc1(x)
        y = self.activation(y)
        y = self.fc2(y)
        return y if not self.return_residual else (y, x)

#@title Block layer (Hyena + MLP layers)

"""
A block consists of a Mixer layer (Hyena or attention), and a MLP layer.

"""

class LinearResidual(nn.Linear):
    """Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.
    """

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return super().forward(input), input

class Block(nn.Module):

    def __init__(self, dim, mixer_cls=None, mlp_cls=None, norm_cls=nn.LayerNorm,
                 dropout_cls=nn.Dropout, prenorm=True, resid_dropout1=0., resid_dropout2=0.,
                 drop_path1=0., drop_path2=0.,
                 return_residual=False,
                 residual_in_fp32=False):
        """
        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/block.py
        For prenorm=True, this Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both
        the hidden_states (output of the MLP) and the residual.
        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        For prenorm=False, this Block has the same structure as a regular postnorm Transformer
        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.
        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.
        This is for performance reason: for post-norm architecture, returning the input allows us
        to fuse the backward of nn.Linear with the residual connection.
        """
        super().__init__()
        self.prenorm = prenorm
        self.return_residual = return_residual
        self.residual_in_fp32 = residual_in_fp32
        if self.residual_in_fp32:
            assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'
        if mixer_cls is None:
            mixer_cls = partial(MHA, num_heads=dim // 64)
        if mlp_cls is None:
            mlp_cls = partial(Mlp, hidden_features=4 * dim)
        self.mixer = mixer_cls()
        self.dropout1 = dropout_cls(resid_dropout1)
        self.drop_path1 = StochasticDepth(drop_path1, mode='row')
        self.norm1 = norm_cls(dim)
        self.mlp = mlp_cls(dim)
        if not isinstance(self.mlp, nn.Identity):
            self.dropout2 = dropout_cls(resid_dropout2)
            self.drop_path2 = StochasticDepth(drop_path2, mode='row')
            self.norm2 = norm_cls(dim)

    def forward(self, hidden_states, residual = None,
                mixer_subset=None, mixer_kwargs=None):
        r"""Pass the input through the encoder layer.
        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))
            mixer_subset: for cross-attention only. If not None, will take a subset of x
                before applying the query projection. Useful for e.g., ViT where we only care
                about the CLS token in the last layer.
        """
        if self.prenorm:
            dropped = self.drop_path1(self.dropout1(hidden_states))
            residual = (dropped + residual) if residual is not None else dropped
            hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
            if mixer_kwargs is None:
                mixer_kwargs = {}
            if mixer_subset is not None:
                mixer_kwargs['mixer_subset'] = mixer_subset
            hidden_states = self.mixer(hidden_states, **mixer_kwargs)
            if mixer_subset is not None:
                residual = residual[:, mixer_subset]
            if not isinstance(self.mlp, nn.Identity):
                dropped = self.drop_path2(self.dropout2(hidden_states))
                residual = (dropped + residual) if residual is not None else dropped
                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))
                if self.residual_in_fp32:
                    residual = residual.to(torch.float32)

                hidden_states = self.mlp(hidden_states)
            return hidden_states, residual
        else:
            assert residual is None
            mixer_out = self.mixer(
                hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {})
            )
            if self.return_residual:  # mixer out is actually a pair here
                mixer_out, hidden_states = mixer_out

            hidden_states = self.norm1((self.drop_path1(self.dropout1(mixer_out))
                                        + hidden_states).to(dtype=self.norm1.weight.dtype))

            if not isinstance(self.mlp, nn.Identity):
                mlp_out = self.mlp(hidden_states)
                if self.return_residual:  # mlp out is actually a pair here
                    mlp_out, hidden_states = mlp_out

                hidden_states = self.norm2((self.drop_path2(self.dropout2(mlp_out))
                                            + hidden_states).to(dtype=self.norm2.weight.dtype))

            return hidden_states

def create_mixer_cls(layer=None,
                     attn_layer_idx=None, attn_cfg=None, layer_idx=None,
                     device=None, dtype=None):
    factory_kwargs = {'device': device, 'dtype': dtype}
    if attn_layer_idx is not None and layer_idx in attn_layer_idx:
        causal = True if attn_cfg is None else attn_cfg.pop('causal', True)

        mha_cls = MHA

        mixer_cls = partial(mha_cls, causal=causal, layer_idx=layer_idx,
                            **(attn_cfg if attn_cfg is not None else {}),**factory_kwargs)
    else:
        # mixer_cls = instantiate(registry.layer, layer, partial=True, layer_idx=layer_idx, **factory_kwargs)

        mixer_cls = partial(HyenaOperator, **layer)

    return mixer_cls

def create_mlp_cls(d_model, d_inner=None, device=None, dtype=None):
    factory_kwargs = {'device': device, 'dtype': dtype}
    inner_dim = d_inner if d_inner is not None else 4 * d_model

    mlp_cls = partial(Mlp, hidden_features=inner_dim,
                          activation=partial(F.gelu, approximate='tanh'), **factory_kwargs)

    return mlp_cls


def create_block(d_model, d_inner=None,
                 layer=None, attn_layer_idx=None,
                 attn_cfg=None, layer_norm_epsilon=1e-5,
                 resid_dropout1=0.0, resid_dropout2=0.0, residual_in_fp32=False,
                 layer_idx=None,
                 device=None, dtype=None):
    factory_kwargs = {'device': device, 'dtype': dtype}
    mixer_cls = create_mixer_cls(layer=layer,
                                 attn_layer_idx=attn_layer_idx,
                                 attn_cfg=attn_cfg, layer_idx=layer_idx,
                                 **factory_kwargs)
    mlp_cls = create_mlp_cls(d_model, d_inner=d_inner,
                             **factory_kwargs)
    norm_cls = partial(nn.LayerNorm, eps=layer_norm_epsilon, **factory_kwargs)
    block = Block(d_model, mixer_cls, mlp_cls, norm_cls=norm_cls,
                  prenorm=True, resid_dropout1=resid_dropout1, resid_dropout2=resid_dropout2,residual_in_fp32=residual_in_fp32)
    block.layer_idx = layer_idx
    return block


# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True,
                  glu_act=False):
    if isinstance(module, nn.Linear):
        nn.init.normal_(module.weight, std=initializer_range)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, std=initializer_range)

    if rescale_prenorm_residual:
        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.
        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
        #
        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
        for name, p in module.named_parameters():
            if name in ["out_proj.weight", "fc2.weight"]:
                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
                nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))
            # If using GLU activation for now, we scale the std by 2
            elif name in ["output_linear.0.weight"]:
                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
                if not glu_act:
                    nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))
                else:
                    out_features = p.shape[0]
                    # Multiplying the first half of the matrix by 2 since sigmoid scales it down by 0.5
                    # on average.
                    nn.init.normal_(p[:out_features // 2], mean=0.0, std=initializer_range / math.sqrt(2 * n_layer) * 2)

#@title Backbone model (stack of blocks)

"""
A backbone model consists of a stack of blocks. If you use attention, then
positional embeddings are included. When using Hyena, then the pos emb
revert to doing nothing.
"""

class GPT2Embeddings(nn.Module):

    def __init__(self, embed_dim, vocab_size, max_position_embeddings, padding_idx=None,
                 word_embed_proj_dim=None, device=None, dtype=None):
        """
            If max_position_embeddings <= 0, there's no position embeddings
            If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension
                the project up to embed_dim
        """
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        if word_embed_proj_dim is None:
            self.word_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx,
                                                **factory_kwargs)
            self.project_in = None
        else:
            self.word_embeddings = nn.Embedding(vocab_size, word_embed_proj_dim,
                                                padding_idx=padding_idx, **factory_kwargs)
            self.project_in = nn.Linear(word_embed_proj_dim, embed_dim, bias=False,
                                        **factory_kwargs)
        self.max_position_embeddings = max_position_embeddings
        if self.max_position_embeddings > 0:
            self.position_embeddings = nn.Embedding(max_position_embeddings, embed_dim,
                                                    **factory_kwargs)

    def forward(self, input_ids, position_ids=None):
        """
            input_ids: (batch, seqlen)
            position_ids: (batch, seqlen)
        """
        batch_size, seqlen = input_ids.shape
        embeddings = self.word_embeddings(input_ids)
        if self.project_in is not None:
            embeddings = self.project_in(embeddings)
        if self.max_position_embeddings > 0:
            if position_ids is None:
                position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)
            position_embeddings = self.position_embeddings(position_ids)
            embeddings = embeddings + position_embeddings
        return embeddings

class LMBackbone(nn.Module):

    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,
                 process_group=None, layer=None,
                 attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,
                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,
                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,
                 device=None, dtype=None, **kwargs) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.process_group = process_group
        self.residual_in_fp32 = residual_in_fp32
        # note max_position_embeddings is 0 for Hyena, and therefore isn't used
        self.embeddings = GPT2Embeddings(d_model, vocab_size, max_position_embeddings,
                                             **factory_kwargs)

        self.layers = nn.ModuleList([create_block(
            d_model, d_inner=d_inner,
            layer=layer, attn_layer_idx=attn_layer_idx,
            attn_cfg=attn_cfg, layer_norm_epsilon=layer_norm_epsilon,
            resid_dropout1=embed_dropout if i == 0 else resid_dropout,
            resid_dropout2=resid_dropout, residual_in_fp32=residual_in_fp32,layer_idx=i,
            **factory_kwargs,
        ) for i in range(n_layer)])

        self.drop_f = nn.Dropout(resid_dropout)
        self.ln_f = nn.LayerNorm(d_model, eps=layer_norm_epsilon, **factory_kwargs)

        self.apply(partial(_init_weights, n_layer=n_layer,
                           **(initializer_cfg if initializer_cfg is not None else {})))

    def forward(self, input_ids, position_ids=None):
        hidden_states = self.embeddings(input_ids, position_ids=position_ids,)
        residual = None

        for layer in self.layers:
            hidden_states, residual = layer(hidden_states, residual)

        dropped = self.drop_f(hidden_states)
        residual = (dropped + residual) if residual is not None else dropped
        hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))

        return hidden_states

#@title Decoder head layer

"""
adapted decode head for binary readmission classification
"""


class SequenceDecoder(nn.Module):
    def __init__(
        self, d_model, d_output=None, l_output=None, use_lengths=False, mode="last"
    ):
        super().__init__()

        self.output_transform = nn.Identity() if d_output is None else nn.Linear(d_model, d_output)

        if l_output is None:
            self.l_output = None
            self.squeeze = False
        elif l_output == 0:
            # Equivalent to getting an output of length 1 and then squeezing
            self.l_output = 1
            self.squeeze = True
        else:
            assert l_output > 0
            self.l_output = l_output
            self.squeeze = False

        self.use_lengths = use_lengths
        self.mode = mode

        if mode == 'ragged':
            assert not use_lengths

    def forward(self, x, state=None, lengths=None, l_output=None):
        """
        x: (n_batch, l_seq, d_model)
        Returns: (n_batch, l_output, d_output)
        """

        if self.l_output is None:
            if l_output is not None:
                assert isinstance(l_output, int)  # Override by pass in
            else:
                # Grab entire output
                l_output = x.size(-2)
            squeeze = False
        else:
            l_output = self.l_output
            squeeze = self.squeeze

        if self.mode == "last":
            restrict = lambda x: x[..., -l_output:, :]
        elif self.mode == "first":
            restrict = lambda x: x[..., :l_output, :]
        elif self.mode == "pool":
            restrict = lambda x: (
                torch.cumsum(x, dim=-2)
                / torch.arange(
                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype
                ).unsqueeze(-1)
            )[..., -l_output:, :]

            def restrict(x):
                L = x.size(-2)
                s = x.sum(dim=-2, keepdim=True)
                if l_output > 1:
                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)
                    c = F.pad(c, (0, 0, 1, 0))
                    s = s - c  # (B, l_output, D)
                    s = s.flip(-2)
                denom = torch.arange(
                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device
                )
                s = s / denom
                return s

        elif self.mode == "sum":
            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]
            # TODO use same restrict function as pool case
        elif self.mode == 'ragged':
            assert lengths is not None, "lengths must be provided for ragged mode"
            # remove any additional padding (beyond max length of any sequence in the batch)
            restrict = lambda x: x[..., : max(lengths), :]
        else:
            raise NotImplementedError(
                "Mode must be ['last' | 'first' | 'pool' | 'sum']"
            )

        # Restrict to actual length of sequence
        if self.use_lengths:
            assert lengths is not None
            x = torch.stack(
                [
                    restrict(out[..., :length, :])
                    for out, length in zip(torch.unbind(x, dim=0), lengths)
                ],
                dim=0,
            )
        else:
            x = restrict(x)

        if squeeze:
            assert x.size(-2) == 1
            x = x.squeeze(-2)

        x = self.output_transform(x)

        return x

    def step(self, x, state=None):
        # Ignore all length logic
        return self.output_transform(x)

#@title Model (backbone + head)

"""
Putting it all together, the model consists of a backbone model
and a decoder head

"""

class ClinicalHeyna(nn.Module):

    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,
                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,
                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,
                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,
                 pad_vocab_size_multiple: int = 1, use_head=None, n_classes: int = 2,
                 device=None, dtype=None, **kwargs) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        if vocab_size % pad_vocab_size_multiple != 0:
            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)

        self.use_head = use_head

        # check if layer (config) has d_model (HF code differs from main Safari code)
        if 'd_model' not in layer:
            layer['d_model'] = d_model

        self.backbone = LMBackbone(
            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,
            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,
            max_position_embeddings=max_position_embeddings,
            resid_dropout=resid_dropout, embed_dropout=embed_dropout,
            layer_norm_epsilon=layer_norm_epsilon,
            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,
            **factory_kwargs, **kwargs
        )

        # we only need a head if doing classification, otherwise we'll use the
        # hidden states as embeddings
        if self.use_head == 'classification':
            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')
        if self.use_head == 'generation':
            self.head = SequenceDecoder(d_model=d_model, d_output=vocab_size, l_output=0, mode='pool')
        # Initialize weights and apply final processing
        self.apply(partial(_init_weights, n_layer=n_layer,
                           **(initializer_cfg if initializer_cfg is not None else {})))

        # if self.use_head:
        #     self.tie_weights()

    # def tie_weights(self):
    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight

    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface
        hidden_states = self.backbone(input_ids, position_ids=position_ids)
        return self.head(hidden_states)

"""# Data pipeline"""

#@title Tokenizer

from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

#@title Utility
import os
import pandas as pd

def save_model(model,path='/content/trained_model.pt'):
  torch.save(model.state_dict(), path)

def load_model(model,path='/content/trained_model.pt'):
  model.load_state_dict(torch.load(path))

def print_statistics(df_input):
  l = list(df_input['TEXT'])
  lenl = [len(tokenizer(i)['input_ids']) for i in l]
  mean = sum(lenl)/len(lenl)

  print(f'dataset size: {len(lenl)}')
  print(f'maximum length: {max(lenl)}')
  print(f'mean: {mean}')
  print(f'std_dev: {(sum([((x - mean) ** 2) for x in lenl]) / len(lenl))**0.5 }')
  return l

#@title Utility: Load Pre-trained Weights (Experiment 1)

def load_pretrained_model():
  model_cfg = '/content/drive/MyDrive/heyna_data/heyna_checkpoint/hyena_small_150b.yaml'
  model_ckpt = '/content/drive/MyDrive/heyna_data/heyna_checkpoint/hyena_small_150b_tok.ckpt'

  loaded_ckpt = torch.load(model_ckpt,map_location=torch.device('cpu'))
  config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)['model_config']
  generation_model = ClinicalHeyna(**config, use_head='generation')

  # binary classification model with the same config to be used later for readmission
  classification_model = ClinicalHeyna(**config, use_head='classification', n_classes=2)
  generation_dict = generation_model.state_dict()

  # ******* load THE PILE 150b pre-trained weights (generation) *****
  for key, value in generation_dict.items():
    if key in['head.output_transform.bias']:
      continue
    if key in ['head.output_transform.weight']:
      generation_dict[key] = loaded_ckpt['lm_head.weight']
    else:
      generation_dict[key] = loaded_ckpt[key]

  generation_model.load_state_dict(generation_dict)
  return generation_model, classification_model

#@title Finetuning data inspection
train = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/discharge/train.csv')
val = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/discharge/val.csv')
test = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/discharge/test.csv')
train3 = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/3days/train.csv')
val3 = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/3days/val.csv')
test3 = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/3days/test.csv')
test2 = pd.read_csv('/content/drive/MyDrive/heyna_data/heyna_finetune_data/2days/test.csv')

train_discharge = pd.concat([train,val],ignore_index=True)
test_discharge = test
train_3 = pd.concat([train3,val3],ignore_index=True)
test_3 = test3
test_2 = test2

l = print_statistics(test_discharge)

#@title Clinical Note dataset

class ClinicalNoteDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        file_path = '/content/drive/MyDrive/heyna_data/clinical_sentences_pretrain.txt',
        d_output=2,
        max_length=5383, # maximum allowed sequence length in the training subset
        tokenizer=None,
        start_idx=0,
        set_size=358809,
    ):

        ''' A next token predition dataset. For each text in file, we use the last token in text as label, and the rest of sequence as input'''

        self.d_output = d_output
        self.tokenizer = tokenizer

        with open(file_path, 'r') as file:
          notes = file.read().split('\n\n')
        notes = [note.strip() for note in notes][start_idx:start_idx+set_size]
        note_toks = [self.tokenizer(n)['input_ids'] for n in notes]

        # generate training subset
        note_toks = [n for n in note_toks if len(n) <= max_length]
        l = [self.last_token(tok) for tok in note_toks]

        self.input_ids = torch.tensor([i[0] + [50257]*(max_length-len(i[0])) for i in l])
        self.targets = torch.tensor([i[1] for i in l])

    def last_token(self,note_tok):
      # return input sequence, last token
      for i, ele in enumerate(reversed(note_tok)):
        if ele != 50257:
            idx = len(note_tok)-i-1
            return note_tok[:idx], ele
      return note_tok[:-1], 50257

    def __len__(self):
        return self.input_ids.shape[0]

    def __getitem__(self, idx):
        return self.input_ids[idx], self.targets[idx]

#@title Readmission dataset

class ReadmissionDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        split,
        max_length=1362,
        folder = 'discharge/', # early note or discharge
        d_output=2, # default binary classification
        tokenizer=None,
        set_size=53567
    ):

        self.tokenizer = tokenizer
        self.d_output = d_output  # needed for decoder to grab
        folder_path = '/content/drive/MyDrive/heyna_data/heyna_finetune_data/' + folder


        if split == 'train':
          train = pd.read_csv(folder_path + split + '.csv')
          val = pd.read_csv(folder_path + 'val.csv')
          data = pd.concat([train, val], ignore_index=True) # train = train + val; we don't use val for anything other than training
        else:
          data = pd.read_csv(folder_path + 'test.csv')

        texts = list(data['TEXT'])[:set_size]
        labels = list(data['Label'])[:set_size]

        note_toks = [self.tokenizer(n,padding="max_length",truncation=True, max_length=max_length)['input_ids'] for n in texts]
        self.input_ids = torch.tensor(note_toks)
        self.labels = torch.tensor(labels).type(torch.LongTensor)

    def __len__(self):
        return self.input_ids.shape[0]

    def __getitem__(self, idx):
        return self.input_ids[idx], self.labels[idx]

"""# Training and Testing"""

# @title Train and Test Loop

def training(model, train_loader, optimizer, epoch, loss_fn, log_interval=10):
    """Training loop."""
    model.train()
    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)

        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
          print()
          print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
              epoch, batch_idx * len(data), len(train_loader.dataset),
              100. * batch_idx / len(train_loader), loss.item()))
          print()

def testing(model, test_loader, loss_fn, output_auroc=False):
    """Test loop."""
    model.eval()
    test_loss = 0
    correct = 0
    y_true = []
    y_probs = []

    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(tqdm(test_loader)):
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += loss_fn(output, target).item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            # print(pred)
            correct += pred.eq(target.view_as(pred)).sum().item()

            prob_output = F.softmax(output, dim=1)
            # print(prob_output)
            class_one_probs = prob_output[:,1].clone()
            y_true.extend(target.cpu().numpy())
            y_probs.extend(class_one_probs.cpu().numpy())

    test_loss /= len(test_loader.dataset)

    if output_auroc:
      try:
        auroc = roc_auc_score(y_true, y_probs)
        print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), AUROC: {:.4f}\n'.format(
            test_loss, correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset), auroc))
      except ValueError:
        print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    else:
      print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

#@title Train on Note
def train_on_note(model, max_length=2048, batch_size=1, training_size=358809, num_epochs=3):

    # experiment settings:
    learning_rate = 6e-4  # good default for Hyena
    weight_decay = 0.1
    n_classes = 2

    tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # clinical note set
    cn_train = ClinicalNoteDataset(
        max_length = max_length,
        tokenizer=tokenizer,
        set_size=training_size
    )


    cn_test = ClinicalNoteDataset(
          max_length = max_length,
          tokenizer=tokenizer,
          start_idx= training_size-1,
          set_size= 1000
      )

    cn_loader = DataLoader(cn_train, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(cn_test, batch_size=batch_size, shuffle=True)
    print(f'\n Effective dataset size: {len(cn_loader.dataset)} \n')

    # loss function
    loss_fn = nn.CrossEntropyLoss()

    # create optimizer
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    model.to(device)

    for epoch in tqdm(range(num_epochs)):
        print()
        print()
        print(f"------------- start of epoch {epoch+1} ----------------")
        training(model, cn_loader, optimizer, epoch, loss_fn)
        testing(model, test_loader, loss_fn, output_auroc=False)
        optimizer.step()
        print()
        print()

#@title Test on Note
def test_on_note(model, max_length=2048, batch_size=1, training_size=358809, num_epochs=3):

    # experiment settings:
    learning_rate = 6e-4  # good default for Hyena
    weight_decay = 0.1
    n_classes = 2

    tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    cn_test = ClinicalNoteDataset(
          max_length = max_length,
          tokenizer=tokenizer,
          start_idx= training_size-1,
          set_size= 1000
      )

    test_loader = DataLoader(cn_test, batch_size=batch_size, shuffle=True)
    print(f'\n Effective test size: {len(test_loader.dataset)} \n')

    model.to(device)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in tqdm(range(num_epochs)):
        print()
        print()
        print(f"------------- start of epoch {epoch+1} ----------------")
        testing(model, test_loader, loss_fn, output_auroc=False)
        print()
        print()

#@title Finetune on Readmission
def finetune_readmission(gmodel, cmodel, training_size, transfer_weight, max_length, num_epochs=3, early_or_discharge='discharge/'):

    if transfer_weight:
      # transfer weight from gmodel to cmodel
      gdict = gmodel.state_dict()
      cdict = cmodel.state_dict()

      for key, value in cdict.items():
        if key in['head.output_transform.bias','head.output_transform.weight']:
          continue
        else:
          cdict[key] = gdict[key]
      cmodel.load_state_dict(cdict)

    # experiment settings:
    batch_size = 1
    learning_rate = 6e-5
    weight_decay = 0.1
    n_classes = 2

    tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # clinical note set
    re_train = ReadmissionDataset(
        split='train',
        max_length = max_length,
        tokenizer=tokenizer,
        folder=early_or_discharge,
        set_size=training_size
    )

    re_test = ReadmissionDataset(
        split='test',
        max_length = max_length,
        tokenizer=tokenizer,
        folder=early_or_discharge,
        set_size=10000
    )

    re_train_loader = DataLoader(re_train, batch_size=batch_size, shuffle=True)
    re_test_loader = DataLoader(re_test, batch_size=batch_size, shuffle=True)

    # loss function
    loss_fn = nn.CrossEntropyLoss()

    # create optimizer
    optimizer = optim.AdamW(cmodel.parameters(), lr=learning_rate, weight_decay=weight_decay)

    cmodel.to(device)

    for epoch in tqdm(range(num_epochs)):
        print()
        print()
        print(f"------------- start of epoch {epoch+1} ----------------")
        training(cmodel, re_train_loader, optimizer, epoch, loss_fn)
        # testing(cmodel, re_train_loader, loss_fn, output_auroc=True)
        testing(cmodel, re_test_loader, loss_fn, output_auroc=True)
        optimizer.step()
        print()
        print()

#@title Test on Readmission
def test_readmission(cmodel, test_size, max_length, num_epochs=3, early_or_discharge='discharge/'):

    # experiment settings:
    batch_size = 1
    weight_decay = 0.1
    n_classes = 2

    tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    re_test = ReadmissionDataset(
        split='test',
        max_length = max_length,
        tokenizer=tokenizer,
        folder=early_or_discharge,
        set_size=test_size
    )

    re_test_loader = DataLoader(re_test, batch_size=batch_size, shuffle=True)

    # loss function
    loss_fn = nn.CrossEntropyLoss()

    cmodel.to(device)

    for epoch in tqdm(range(num_epochs)):
        print()
        print()
        print(f"------------- start of epoch {epoch+1} ----------------")
        testing(cmodel, re_test_loader, loss_fn, output_auroc=True)
        print()
        print()

"""# Experiments"""

#@title Experiment 1 train
gmodel, cmodel = load_pretrained_model()

gmodel_cpy = copy.deepcopy(gmodel) # pre-trained heyna models before ClinicalHeyna training
cmodel_cpy = copy.deepcopy(cmodel)

# base model without pretraining
test_on_note(gmodel_cpy, max_length=2048,training_size=30000,batch_size=1,num_epochs=1)

# go back to base model

# gmodel = copy.deepcopy(gmodel_cpy)
# cmodel = copy.deepcopy(cmodel_cpy)

# train gmodel on clinical notes
train_on_note(gmodel, max_length=2048,training_size=30000,batch_size=1,num_epochs=5)

gmodel_pretrained_cpy = copy.deepcopy(gmodel)

# backup the pretrained weights
save_model(gmodel,'gmodel.pt')

#@title Experiment 1 finetune
load_model(gmodel,'/content/drive/MyDrive/gmodel5.pt')

finetune_readmission(gmodel, cmodel, training_size=10000, transfer_weight=True, max_length=2048, early_or_discharge='3days/',num_epochs=3)

cmodel_finetuned_cpy = copy.deepcopy(cmodel)

save_model(cmodel,'cmodel_early.pt')

# baseline Heyna
test_readmission(cmodel_cpy, test_size=10000, max_length=2048, early_or_discharge='3days/',num_epochs=1)

test_readmission(cmodel_cpy, test_size=10000, max_length=2048, early_or_discharge='2days/',num_epochs=1)

# finetuned clinicalHeyna
# test_readmission(cmodel, test_size=10000, max_length=2048, early_or_discharge='3days/',num_epochs=1)

test_readmission(cmodel, test_size=10000, max_length=2048, early_or_discharge='2days/',num_epochs=1)

#@title Experiment 2 finetune
finetune_readmission(gmodel, cmodel, training_size=10000, transfer_weight=True, max_length=2048, early_or_discharge='discharge/',num_epochs=2)

test_readmission(cmodel_cpy, test_size=10000, max_length=2048, early_or_discharge='discharge/',num_epochs=1)

test_readmission(cmodel, test_size=10000, max_length=2048, early_or_discharge='discharge/',num_epochs=1)